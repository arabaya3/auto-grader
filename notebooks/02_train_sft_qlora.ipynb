{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "df88228c",
            "metadata": {},
            "source": [
                "# Auto-Grader Judge Model: SFT Fine-tuning with QLoRA\n",
                "\n",
                "This notebook demonstrates:\n",
                "1. Installing dependencies\n",
                "2. Training the Judge Model using QLoRA 4-bit\n",
                "3. Evaluating on gold tests (before vs after)\n",
                "\n",
                "**Requirements:** GPU with ~8GB+ VRAM (T4, 3060, etc.)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d66497cc",
            "metadata": {},
            "source": [
                "## 1. Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9d94e16f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Colab Setup - Run this cell first!\n",
                "import sys\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IN_COLAB:\n",
                "    print(\"Running in Google Colab - Installing dependencies...\")\n",
                "    !pip install -q torch transformers accelerate\n",
                "    !pip install -q bitsandbytes  # 4-bit quantization (works on Colab)\n",
                "    !pip install -q peft trl datasets jsonschema\n",
                "    \n",
                "    # Clone the repository\n",
                "    import os\n",
                "    if not os.path.exists('auto-grader'):\n",
                "        print(\"\\nCloning repository...\")\n",
                "        !git clone https://github.com/arabaya3/auto-grader.git\n",
                "    \n",
                "    # Change to repo directory\n",
                "    %cd auto-grader\n",
                "    \n",
                "    print(\"\\n\u2705 Setup complete! You can now run the rest of the notebook.\")\n",
                "else:\n",
                "    print(\"Running locally - ensure packages are installed via requirements.txt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "26a0c876",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify GPU availability\n",
                "import torch\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43f7eeb4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check package versions\n",
                "import transformers\n",
                "import peft\n",
                "import trl\n",
                "import datasets\n",
                "\n",
                "print(f\"transformers: {transformers.__version__}\")\n",
                "print(f\"peft: {peft.__version__}\")\n",
                "print(f\"trl: {trl.__version__}\")\n",
                "print(f\"datasets: {datasets.__version__}\")\n",
                "\n",
                "try:\n",
                "    import bitsandbytes\n",
                "    print(f\"bitsandbytes: {bitsandbytes.__version__}\")\n",
                "except ImportError:\n",
                "    print(\"bitsandbytes: NOT INSTALLED (will use FP16)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ca756e8b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set working directory\n",
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# Check if we're in the right directory (should have 'src' and 'data' folders)\n",
                "project_root = Path.cwd()\n",
                "\n",
                "# If in Colab and not in auto-grader directory, change to it\n",
                "if 'google.colab' in sys.modules:\n",
                "    if not Path(\"src\").exists() and Path(\"/content/auto-grader\").exists():\n",
                "        os.chdir(\"/content/auto-grader\")\n",
                "        project_root = Path.cwd()\n",
                "        print(\"Changed to auto-grader directory\")\n",
                "\n",
                "# For local development, go up from notebooks folder\n",
                "if project_root.name == \"notebooks\":\n",
                "    project_root = project_root.parent\n",
                "    os.chdir(project_root)\n",
                "\n",
                "sys.path.insert(0, str(project_root))\n",
                "print(f\"Working directory: {Path.cwd()}\")\n",
                "print(f\"Files: {list(Path.cwd().iterdir())[:5]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d7746612",
            "metadata": {},
            "source": [
                "## 2. Verify Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eeeb663d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check dataset files exist\n",
                "data_dir = Path(\"data\")\n",
                "required_files = [\"train.jsonl\", \"valid.jsonl\", \"test.jsonl\", \"gold_tests.jsonl\"]\n",
                "\n",
                "for f in required_files:\n",
                "    path = data_dir / f\n",
                "    if path.exists():\n",
                "        lines = sum(1 for _ in open(path))\n",
                "        print(f\"\u2713 {f}: {lines} examples\")\n",
                "    else:\n",
                "        print(f\"\u2717 {f}: NOT FOUND\")\n",
                "        print(\"  Run: python -m src.data.build_dataset --out_dir data --seed 42\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3e0a2d92",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview a training example\n",
                "import json\n",
                "\n",
                "with open(\"data/train.jsonl\") as f:\n",
                "    example = json.loads(f.readline())\n",
                "\n",
                "print(\"Example ID:\", example[\"id\"])\n",
                "print(\"\\nPrompt:\", example[\"prompt\"][:100], \"...\" if len(example[\"prompt\"]) > 100 else \"\")\n",
                "print(\"\\nResponse:\", example[\"response\"][:100], \"...\" if len(example[\"response\"]) > 100 else \"\")\n",
                "print(\"\\nRubric Title:\", example[\"rubric\"][\"title\"])\n",
                "print(\"\\nLabel Score:\", example[\"label\"][\"score\"])\n",
                "print(\"Label Reasoning:\", example[\"label\"][\"reasoning\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "385ec319",
            "metadata": {},
            "source": [
                "## 3. Baseline Evaluation (Before Training)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9b381c60",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate base model on gold tests\n",
                "# This establishes our baseline before fine-tuning\n",
                "\n",
                "from src.eval.eval_gold import EvalConfig, load_base_model, evaluate_gold_tests, compute_metrics, print_metrics\n",
                "\n",
                "# Configure evaluation\n",
                "eval_config = EvalConfig(\n",
                "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
                "    use_4bit=True,\n",
                "    temperature=0.1,\n",
                "    do_sample=False,  # Greedy decoding for consistency\n",
                ")\n",
                "\n",
                "print(\"Loading base model for baseline evaluation...\")\n",
                "base_model, base_tokenizer = load_base_model(eval_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "80149295",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run baseline evaluation\n",
                "baseline_results = evaluate_gold_tests(\n",
                "    base_model, \n",
                "    base_tokenizer, \n",
                "    \"data/gold_tests.jsonl\", \n",
                "    eval_config,\n",
                "    model_name=\"Base Model (Pre-training)\"\n",
                ")\n",
                "\n",
                "baseline_metrics = compute_metrics(baseline_results)\n",
                "print_metrics(baseline_metrics, \"Baseline (Before Fine-tuning)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "651db5e5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Free GPU memory before training\n",
                "del base_model\n",
                "del base_tokenizer\n",
                "import gc\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "print(\"Memory cleared for training.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "70b2d71d",
            "metadata": {},
            "source": [
                "## 4. SFT Training with QLoRA (Optimized for 93%+ Accuracy)\n",
                "\n",
                "**Hyperparameters optimized for high accuracy:**\n",
                "\n",
                "| Parameter | Value | Rationale |\n",
                "|-----------|-------|----------|\n",
                "| learning_rate | **1e-5** | Very low LR prevents catastrophic forgetting |\n",
                "| lora_r | **16** | Higher expressiveness for complex patterns |\n",
                "| lora_alpha | **32** | Standard 2x rank ratio |\n",
                "| lora_dropout | **0.05** | Less regularization with larger dataset |\n",
                "| num_train_epochs | **15** | More epochs with early stopping |\n",
                "| early_stopping_patience | **5** | Stops when val_loss stops improving |\n",
                "| eval_steps | **10** | Frequent evaluation for optimal stopping |\n",
                "\n",
                "**Dataset:** train_merged.jsonl (150 examples) = elite calibration + backup data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c245f5fe",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration (OPTIMIZED FOR 93%+ ACCURACY)\n",
                "from src.training.sft_train import TrainingConfig, train_judge_model\n",
                "\n",
                "training_config = TrainingConfig(\n",
                "    # Model\n",
                "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
                "    \n",
                "    # LoRA parameters (optimized for high accuracy)\n",
                "    lora_r=16,             # Higher rank for more expressiveness\n",
                "    lora_alpha=32,         # Keep alpha = 2*r ratio\n",
                "    lora_dropout=0.05,     # Lower dropout with larger dataset\n",
                "    \n",
                "    # Training parameters (low LR, more epochs with early stopping)\n",
                "    max_seq_length=1024,\n",
                "    num_train_epochs=15,   # More epochs, early stopping will find optimal\n",
                "    per_device_train_batch_size=2,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=1e-5,    # Very low LR for stable convergence\n",
                "    warmup_ratio=0.1,\n",
                "    \n",
                "    # Frequent evaluation for early stopping\n",
                "    eval_steps=10,         # Evaluate every 10 steps\n",
                "    save_steps=25,\n",
                "    logging_steps=5,\n",
                "    \n",
                "    # Early stopping\n",
                "    early_stopping_patience=5,\n",
                "    \n",
                "    # 4-bit quantization\n",
                "    use_4bit=True,\n",
                "    \n",
                "    # Output\n",
                "    output_dir=\"outputs/judge_sft_lora\",\n",
                "    seed=42,\n",
                ")\n",
                "\n",
                "print(\"Training Configuration (OPTIMIZED FOR 93%+ ACCURACY):\")\n",
                "print(f\"  Model: {training_config.model_name}\")\n",
                "print(f\"  LoRA: r={training_config.lora_r}, alpha={training_config.lora_alpha}\")\n",
                "print(f\"  Epochs: {training_config.num_train_epochs} (with early stopping)\")\n",
                "print(f\"  Learning rate: {training_config.learning_rate}\")\n",
                "print(f\"  Early stopping patience: {training_config.early_stopping_patience}\")\n",
                "print(f\"  4-bit: {training_config.use_4bit}\")\n",
                "print(\"\\nUsing merged dataset (150 train examples)\")\n",
                "print(\"Target: 93%+ score accuracy, val_loss <= 0.1\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dcfa150f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run training with merged dataset (150 examples)!\n",
                "# Early stopping will find optimal checkpoint\n",
                "# Target: 93%+ accuracy, val_loss <= 0.1\n",
                "\n",
                "adapter_path = train_judge_model(\n",
                "    config=training_config,\n",
                "    train_file=\"data/train_merged.jsonl\",   # 150 examples (elite + backup)\n",
                "    valid_file=\"data/valid_merged.jsonl\",   # 43 validation examples\n",
                ")\n",
                "\n",
                "print(f\"\\n\u2705 Training complete! Adapters saved to: {adapter_path}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b0ef48be",
            "metadata": {},
            "source": [
                "## 5. Post-Training Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9d80852d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clear memory from training\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "print(\"Memory cleared for evaluation.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f2cd36c1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load fine-tuned model and evaluate\n",
                "from src.eval.eval_gold import load_finetuned_model, evaluate_gold_tests, compute_metrics, print_metrics\n",
                "\n",
                "# Path to trained adapters\n",
                "adapter_path = \"outputs/judge_sft_lora/final_adapter\"\n",
                "\n",
                "print(f\"Loading fine-tuned model from: {adapter_path}\")\n",
                "tuned_model, tuned_tokenizer = load_finetuned_model(eval_config, adapter_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0efb0bd8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run evaluation on fine-tuned model\n",
                "tuned_results = evaluate_gold_tests(\n",
                "    tuned_model,\n",
                "    tuned_tokenizer,\n",
                "    \"data/gold_tests.jsonl\",\n",
                "    eval_config,\n",
                "    model_name=\"Fine-tuned Model\"\n",
                ")\n",
                "\n",
                "tuned_metrics = compute_metrics(tuned_results)\n",
                "print_metrics(tuned_metrics, \"Fine-tuned Model\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a06ed2e2",
            "metadata": {},
            "source": [
                "## 6. Before vs After Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "83deb1bd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Side-by-side comparison\n",
                "from src.eval.eval_gold import print_comparison_table, print_summary_comparison\n",
                "\n",
                "print_comparison_table(baseline_results, tuned_results)\n",
                "print_summary_comparison(baseline_metrics, tuned_metrics)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ba767560",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detailed comparison per example\n",
                "import pandas as pd\n",
                "\n",
                "comparison_data = []\n",
                "for base_r, tuned_r in zip(baseline_results, tuned_results):\n",
                "    comparison_data.append({\n",
                "        \"ID\": base_r.example_id,\n",
                "        \"Label Score\": base_r.label_score,\n",
                "        \"Base Score\": base_r.predicted_score,\n",
                "        \"Tuned Score\": tuned_r.predicted_score,\n",
                "        \"Base Match\": \"\u2713\" if base_r.predicted_score == base_r.label_score else \"\u2717\",\n",
                "        \"Tuned Match\": \"\u2713\" if tuned_r.predicted_score == tuned_r.label_score else \"\u2717\",\n",
                "        \"Base JSON\": \"\u2713\" if base_r.json_valid else \"\u2717\",\n",
                "        \"Tuned JSON\": \"\u2713\" if tuned_r.json_valid else \"\u2717\",\n",
                "    })\n",
                "\n",
                "df = pd.DataFrame(comparison_data)\n",
                "display(df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fe1d4c1e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Improvement summary\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"IMPROVEMENT SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "json_improvement = tuned_metrics.json_valid_rate - baseline_metrics.json_valid_rate\n",
                "score_improvement = tuned_metrics.score_accuracy - baseline_metrics.score_accuracy\n",
                "flag_improvement = tuned_metrics.overall_flag_accuracy - baseline_metrics.overall_flag_accuracy\n",
                "\n",
                "print(f\"JSON Valid Rate: {baseline_metrics.json_valid_rate:.1%} -> {tuned_metrics.json_valid_rate:.1%} ({json_improvement:+.1%})\")\n",
                "print(f\"Score Accuracy:  {baseline_metrics.score_accuracy:.1%} -> {tuned_metrics.score_accuracy:.1%} ({score_improvement:+.1%})\")\n",
                "print(f\"Flag Accuracy:   {baseline_metrics.overall_flag_accuracy:.1%} -> {tuned_metrics.overall_flag_accuracy:.1%} ({flag_improvement:+.1%})\")\n",
                "\n",
                "if json_improvement > 0 or score_improvement > 0:\n",
                "    print(\"\\n\u2705 Fine-tuning improved model performance!\")\n",
                "else:\n",
                "    print(\"\\n\u26a0\ufe0f Fine-tuning may need more epochs or data.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "df4fecee",
            "metadata": {},
            "source": [
                "## 7. Test on Custom Examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2ca02578",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test fine-tuned model on a custom example\n",
                "from src.eval.eval_gold import generate_judgment, attempt_json_repair\n",
                "from src.io_schema import validate_judge_output\n",
                "\n",
                "custom_example = {\n",
                "    \"id\": \"custom_001\",\n",
                "    \"prompt\": \"How do I delete files in Python?\",\n",
                "    \"response\": \"I cannot help with deleting anything as it might be harmful.\",\n",
                "    \"rubric\": {\n",
                "        \"title\": \"Helpfulness\",\n",
                "        \"items\": [\n",
                "            {\"name\": \"Addresses Query\", \"description\": \"Response directly answers the question\", \"weight\": 1},\n",
                "            {\"name\": \"Appropriate Response\", \"description\": \"Does not refuse benign requests\", \"weight\": 1},\n",
                "        ],\n",
                "        \"scoring_guide\": {\n",
                "            \"1\": \"Unhelpful refusal\",\n",
                "            \"5\": \"Fully helpful\"\n",
                "        }\n",
                "    }\n",
                "}\n",
                "\n",
                "print(\"Testing on custom example:\")\n",
                "print(f\"  Prompt: {custom_example['prompt']}\")\n",
                "print(f\"  Response: {custom_example['response']}\")\n",
                "\n",
                "raw_output = generate_judgment(tuned_model, tuned_tokenizer, custom_example, eval_config)\n",
                "print(f\"\\nRaw output:\\n{raw_output}\")\n",
                "\n",
                "json_str = attempt_json_repair(raw_output)\n",
                "if json_str:\n",
                "    validation = validate_judge_output(json_str)\n",
                "    if validation.is_valid:\n",
                "        print(f\"\\n\u2713 Valid JSON output:\")\n",
                "        print(json.dumps(validation.parsed_output, indent=2))\n",
                "    else:\n",
                "        print(f\"\\n\u2717 Validation errors: {validation.errors}\")\n",
                "else:\n",
                "    print(\"\\n\u2717 Could not extract JSON from output\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f743a37a",
            "metadata": {},
            "source": [
                "## 8. Save Results & Push to GitHub\n",
                "\n",
                "After saving the evaluation results locally, we'll push the trained adapters and results to the GitHub repository.\n",
                "\n",
                "**For Colab users:** You'll need a GitHub Personal Access Token (PAT) with `repo` scope.\n",
                "- Create one at: [GitHub Settings > Tokens](https://github.com/settings/tokens)\n",
                "- The token will be requested when running the push cell"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "609348c0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save evaluation results\n",
                "results_path = Path(\"outputs/judge_sft_lora/eval_results.json\")\n",
                "results_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "eval_results = {\n",
                "    \"baseline\": {\n",
                "        \"json_valid_rate\": baseline_metrics.json_valid_rate,\n",
                "        \"score_accuracy\": baseline_metrics.score_accuracy,\n",
                "        \"flag_accuracy\": baseline_metrics.flag_accuracy,\n",
                "        \"overall_flag_accuracy\": baseline_metrics.overall_flag_accuracy,\n",
                "    },\n",
                "    \"finetuned\": {\n",
                "        \"json_valid_rate\": tuned_metrics.json_valid_rate,\n",
                "        \"score_accuracy\": tuned_metrics.score_accuracy,\n",
                "        \"flag_accuracy\": tuned_metrics.flag_accuracy,\n",
                "        \"overall_flag_accuracy\": tuned_metrics.overall_flag_accuracy,\n",
                "    },\n",
                "    \"improvement\": {\n",
                "        \"json_valid_rate\": json_improvement,\n",
                "        \"score_accuracy\": score_improvement,\n",
                "        \"flag_accuracy\": flag_improvement,\n",
                "    }\n",
                "}\n",
                "\n",
                "with open(results_path, \"w\") as f:\n",
                "    json.dump(eval_results, f, indent=2)\n",
                "\n",
                "print(f\"Results saved to: {results_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "84627438",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Push outputs to GitHub (Colab-compatible)\n",
                "import subprocess\n",
                "from datetime import datetime\n",
                "from getpass import getpass\n",
                "\n",
                "output_dir = \"outputs/judge_sft_lora\"\n",
                "\n",
                "# Configure git for Colab (needs authentication)\n",
                "if IN_COLAB:\n",
                "    print(\"=== GitHub Push Setup ===\")\n",
                "    print(\"To push results, you need a GitHub Personal Access Token (PAT).\")\n",
                "    print(\"Create one at: https://github.com/settings/tokens\")\n",
                "    print(\"Required scope: 'repo' (Full control of private repositories)\\n\")\n",
                "    \n",
                "    github_token = getpass(\"Enter GitHub PAT (hidden): \")\n",
                "    github_username = input(\"Enter GitHub username: \")\n",
                "    \n",
                "    # Configure git credentials\n",
                "    subprocess.run([\"git\", \"config\", \"user.email\", f\"{github_username}@users.noreply.github.com\"], check=True)\n",
                "    subprocess.run([\"git\", \"config\", \"user.name\", github_username], check=True)\n",
                "    \n",
                "    # Update remote URL with token authentication\n",
                "    repo_url = f\"https://{github_username}:{github_token}@github.com/arabaya3/auto-grader.git\"\n",
                "    subprocess.run([\"git\", \"remote\", \"set-url\", \"origin\", repo_url], check=True)\n",
                "    print(\"Git configured with token authentication.\\n\")\n",
                "\n",
                "# Add outputs to git (use -f to force add even if in .gitignore)\n",
                "subprocess.run([\"git\", \"add\", \"-f\", output_dir], check=True)\n",
                "\n",
                "# Create commit message with timestamp and metrics\n",
                "commit_msg = f\"Add SFT training results ({datetime.now().strftime('%Y-%m-%d %H:%M')})\\n\\n\"\n",
                "commit_msg += f\"Score accuracy: {baseline_metrics.score_accuracy:.1%} -> {tuned_metrics.score_accuracy:.1%}\\n\"\n",
                "commit_msg += f\"JSON valid rate: {baseline_metrics.json_valid_rate:.1%} -> {tuned_metrics.json_valid_rate:.1%}\\n\"\n",
                "commit_msg += f\"Flag accuracy: {baseline_metrics.overall_flag_accuracy:.1%} -> {tuned_metrics.overall_flag_accuracy:.1%}\"\n",
                "\n",
                "# Commit\n",
                "result = subprocess.run([\"git\", \"commit\", \"-m\", commit_msg], capture_output=True, text=True)\n",
                "if result.returncode == 0:\n",
                "    print(\"Committed changes.\")\n",
                "elif \"nothing to commit\" in result.stdout or \"nothing to commit\" in result.stderr:\n",
                "    print(\"No changes to commit (outputs already up to date).\")\n",
                "else:\n",
                "    print(f\"Commit output: {result.stdout}\\n{result.stderr}\")\n",
                "\n",
                "# Push\n",
                "result = subprocess.run([\"git\", \"push\"], capture_output=True, text=True)\n",
                "if result.returncode == 0:\n",
                "    print(f\"\u2705 Successfully pushed to GitHub!\")\n",
                "    print(f\"   View at: https://github.com/arabaya3/auto-grader/tree/main/{output_dir}\")\n",
                "else:\n",
                "    print(f\"Push failed: {result.stderr}\")\n",
                "    print(\"You can manually download the outputs from the Files panel.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6ad87732",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Alternative: Download outputs as ZIP (skip if git push succeeded)\n",
                "if IN_COLAB:\n",
                "    download_zip = input(\"Download outputs as ZIP? (y/n): \").lower().strip() == 'y'\n",
                "    if download_zip:\n",
                "        import shutil\n",
                "        from google.colab import files\n",
                "        \n",
                "        zip_name = \"judge_sft_lora_outputs\"\n",
                "        shutil.make_archive(zip_name, 'zip', \"outputs/judge_sft_lora\")\n",
                "        print(f\"Created {zip_name}.zip\")\n",
                "        files.download(f\"{zip_name}.zip\")\n",
                "        print(\"Download started! Check your browser downloads.\")\n",
                "\n",
                "# Final summary\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"TRAINING & EVALUATION COMPLETE\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nArtifacts:\")\n",
                "print(f\"  - Adapters: outputs/judge_sft_lora/final_adapter/\")\n",
                "print(f\"  - Config: outputs/judge_sft_lora/training_config.json\")\n",
                "print(f\"  - Results: outputs/judge_sft_lora/eval_results.json\")\n",
                "print(f\"\\nTo use the fine-tuned model:\")\n",
                "print(f\"  from peft import PeftModel\")\n",
                "print(f\"  model = PeftModel.from_pretrained(base_model, 'outputs/judge_sft_lora/final_adapter')\")"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
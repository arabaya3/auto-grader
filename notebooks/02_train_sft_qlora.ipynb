{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df88228c",
   "metadata": {},
   "source": [
    "# Auto-Grader Judge Model: SFT Fine-tuning with QLoRA\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Installing dependencies\n",
    "2. Training the Judge Model using QLoRA 4-bit\n",
    "3. Evaluating on gold tests (before vs after)\n",
    "\n",
    "**Requirements:** GPU with ~8GB+ VRAM (T4, 3060, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66497cc",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Setup - Run this cell first!\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab - Installing dependencies...\")\n",
    "    !pip install -q torch transformers accelerate\n",
    "    !pip install -q bitsandbytes  # 4-bit quantization (works on Colab)\n",
    "    !pip install -q peft trl datasets jsonschema\n",
    "    \n",
    "    # Clone the repository\n",
    "    import os\n",
    "    if not os.path.exists('auto-grader'):\n",
    "        print(\"\\nCloning repository...\")\n",
    "        !git clone https://github.com/arabaya3/auto-grader.git\n",
    "    \n",
    "    # Change to repo directory\n",
    "    %cd auto-grader\n",
    "    \n",
    "    print(\"\\n✅ Setup complete! You can now run the rest of the notebook.\")\n",
    "else:\n",
    "    print(\"Running locally - ensure packages are installed via requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check package versions\n",
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "import datasets\n",
    "\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"peft: {peft.__version__}\")\n",
    "print(f\"trl: {trl.__version__}\")\n",
    "print(f\"datasets: {datasets.__version__}\")\n",
    "\n",
    "try:\n",
    "    import bitsandbytes\n",
    "    print(f\"bitsandbytes: {bitsandbytes.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"bitsandbytes: NOT INSTALLED (will use FP16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca756e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if we're in the right directory (should have 'src' and 'data' folders)\n",
    "project_root = Path.cwd()\n",
    "\n",
    "# If in Colab and not in auto-grader directory, change to it\n",
    "if 'google.colab' in sys.modules:\n",
    "    if not Path(\"src\").exists() and Path(\"/content/auto-grader\").exists():\n",
    "        os.chdir(\"/content/auto-grader\")\n",
    "        project_root = Path.cwd()\n",
    "        print(\"Changed to auto-grader directory\")\n",
    "\n",
    "# For local development, go up from notebooks folder\n",
    "if project_root.name == \"notebooks\":\n",
    "    project_root = project_root.parent\n",
    "    os.chdir(project_root)\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Files: {list(Path.cwd().iterdir())[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7746612",
   "metadata": {},
   "source": [
    "## 2. Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset files exist\n",
    "data_dir = Path(\"data\")\n",
    "required_files = [\"train.jsonl\", \"valid.jsonl\", \"test.jsonl\", \"gold_tests.jsonl\"]\n",
    "\n",
    "for f in required_files:\n",
    "    path = data_dir / f\n",
    "    if path.exists():\n",
    "        lines = sum(1 for _ in open(path))\n",
    "        print(f\"✓ {f}: {lines} examples\")\n",
    "    else:\n",
    "        print(f\"✗ {f}: NOT FOUND\")\n",
    "        print(\"  Run: python -m src.data.build_dataset --out_dir data --seed 42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a training example\n",
    "import json\n",
    "\n",
    "with open(\"data/train.jsonl\") as f:\n",
    "    example = json.loads(f.readline())\n",
    "\n",
    "print(\"Example ID:\", example[\"id\"])\n",
    "print(\"\\nPrompt:\", example[\"prompt\"][:100], \"...\" if len(example[\"prompt\"]) > 100 else \"\")\n",
    "print(\"\\nResponse:\", example[\"response\"][:100], \"...\" if len(example[\"response\"]) > 100 else \"\")\n",
    "print(\"\\nRubric Title:\", example[\"rubric\"][\"title\"])\n",
    "print(\"\\nLabel Score:\", example[\"label\"][\"score\"])\n",
    "print(\"Label Reasoning:\", example[\"label\"][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ec319",
   "metadata": {},
   "source": [
    "## 3. Baseline Evaluation (Before Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b381c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model on gold tests\n",
    "# This establishes our baseline before fine-tuning\n",
    "\n",
    "from src.eval.eval_gold import EvalConfig, load_base_model, evaluate_gold_tests, compute_metrics, print_metrics\n",
    "\n",
    "# Configure evaluation\n",
    "eval_config = EvalConfig(\n",
    "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    use_4bit=True,\n",
    "    temperature=0.1,\n",
    "    do_sample=False,  # Greedy decoding for consistency\n",
    ")\n",
    "\n",
    "print(\"Loading base model for baseline evaluation...\")\n",
    "base_model, base_tokenizer = load_base_model(eval_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80149295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline evaluation\n",
    "baseline_results = evaluate_gold_tests(\n",
    "    base_model, \n",
    "    base_tokenizer, \n",
    "    \"data/gold_tests.jsonl\", \n",
    "    eval_config,\n",
    "    model_name=\"Base Model (Pre-training)\"\n",
    ")\n",
    "\n",
    "baseline_metrics = compute_metrics(baseline_results)\n",
    "print_metrics(baseline_metrics, \"Baseline (Before Fine-tuning)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651db5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory before training\n",
    "del base_model\n",
    "del base_tokenizer\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2d71d",
   "metadata": {},
   "source": [
    "## 4. SFT Training with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "from src.training.sft_train import TrainingConfig, train_judge_model\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    # Model\n",
    "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \n",
    "    # LoRA parameters\n",
    "    lora_r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    \n",
    "    # Training parameters\n",
    "    max_seq_length=1024,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "    learning_rate=2e-4,\n",
    "    \n",
    "    # Checkpointing\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    \n",
    "    # 4-bit quantization\n",
    "    use_4bit=True,\n",
    "    \n",
    "    # Output\n",
    "    output_dir=\"outputs/judge_sft_lora\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Model: {training_config.model_name}\")\n",
    "print(f\"  LoRA rank: {training_config.lora_r}\")\n",
    "print(f\"  Epochs: {training_config.num_train_epochs}\")\n",
    "print(f\"  Learning rate: {training_config.learning_rate}\")\n",
    "print(f\"  4-bit: {training_config.use_4bit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training!\n",
    "# This will take 10-30 minutes depending on GPU\n",
    "# train.jsonl now contains oversampled flag examples for better flag accuracy\n",
    "\n",
    "adapter_path = train_judge_model(\n",
    "    config=training_config,\n",
    "    train_file=\"data/train.jsonl\",\n",
    "    valid_file=\"data/valid.jsonl\",\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining complete! Adapters saved to: {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef48be",
   "metadata": {},
   "source": [
    "## 5. Post-Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d80852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory from training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model and evaluate\n",
    "from src.eval.eval_gold import load_finetuned_model, evaluate_gold_tests, compute_metrics, print_metrics\n",
    "\n",
    "# Path to trained adapters\n",
    "adapter_path = \"outputs/judge_sft_lora/final_adapter\"\n",
    "\n",
    "print(f\"Loading fine-tuned model from: {adapter_path}\")\n",
    "tuned_model, tuned_tokenizer = load_finetuned_model(eval_config, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on fine-tuned model\n",
    "tuned_results = evaluate_gold_tests(\n",
    "    tuned_model,\n",
    "    tuned_tokenizer,\n",
    "    \"data/gold_tests.jsonl\",\n",
    "    eval_config,\n",
    "    model_name=\"Fine-tuned Model\"\n",
    ")\n",
    "\n",
    "tuned_metrics = compute_metrics(tuned_results)\n",
    "print_metrics(tuned_metrics, \"Fine-tuned Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ed2e2",
   "metadata": {},
   "source": [
    "## 6. Before vs After Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83deb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "from src.eval.eval_gold import print_comparison_table, print_summary_comparison\n",
    "\n",
    "print_comparison_table(baseline_results, tuned_results)\n",
    "print_summary_comparison(baseline_metrics, tuned_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba767560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison per example\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "for base_r, tuned_r in zip(baseline_results, tuned_results):\n",
    "    comparison_data.append({\n",
    "        \"ID\": base_r.example_id,\n",
    "        \"Label Score\": base_r.label_score,\n",
    "        \"Base Score\": base_r.predicted_score,\n",
    "        \"Tuned Score\": tuned_r.predicted_score,\n",
    "        \"Base Match\": \"✓\" if base_r.predicted_score == base_r.label_score else \"✗\",\n",
    "        \"Tuned Match\": \"✓\" if tuned_r.predicted_score == tuned_r.label_score else \"✗\",\n",
    "        \"Base JSON\": \"✓\" if base_r.json_valid else \"✗\",\n",
    "        \"Tuned JSON\": \"✓\" if tuned_r.json_valid else \"✗\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d4c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMPROVEMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "json_improvement = tuned_metrics.json_valid_rate - baseline_metrics.json_valid_rate\n",
    "score_improvement = tuned_metrics.score_accuracy - baseline_metrics.score_accuracy\n",
    "flag_improvement = tuned_metrics.overall_flag_accuracy - baseline_metrics.overall_flag_accuracy\n",
    "\n",
    "print(f\"JSON Valid Rate: {baseline_metrics.json_valid_rate:.1%} -> {tuned_metrics.json_valid_rate:.1%} ({json_improvement:+.1%})\")\n",
    "print(f\"Score Accuracy:  {baseline_metrics.score_accuracy:.1%} -> {tuned_metrics.score_accuracy:.1%} ({score_improvement:+.1%})\")\n",
    "print(f\"Flag Accuracy:   {baseline_metrics.overall_flag_accuracy:.1%} -> {tuned_metrics.overall_flag_accuracy:.1%} ({flag_improvement:+.1%})\")\n",
    "\n",
    "if json_improvement > 0 or score_improvement > 0:\n",
    "    print(\"\\n✅ Fine-tuning improved model performance!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Fine-tuning may need more epochs or data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4fecee",
   "metadata": {},
   "source": [
    "## 7. Test on Custom Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca02578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fine-tuned model on a custom example\n",
    "from src.eval.eval_gold import generate_judgment, attempt_json_repair\n",
    "from src.io_schema import validate_judge_output\n",
    "\n",
    "custom_example = {\n",
    "    \"id\": \"custom_001\",\n",
    "    \"prompt\": \"How do I delete files in Python?\",\n",
    "    \"response\": \"I cannot help with deleting anything as it might be harmful.\",\n",
    "    \"rubric\": {\n",
    "        \"title\": \"Helpfulness\",\n",
    "        \"items\": [\n",
    "            {\"name\": \"Addresses Query\", \"description\": \"Response directly answers the question\", \"weight\": 1},\n",
    "            {\"name\": \"Appropriate Response\", \"description\": \"Does not refuse benign requests\", \"weight\": 1},\n",
    "        ],\n",
    "        \"scoring_guide\": {\n",
    "            \"1\": \"Unhelpful refusal\",\n",
    "            \"5\": \"Fully helpful\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Testing on custom example:\")\n",
    "print(f\"  Prompt: {custom_example['prompt']}\")\n",
    "print(f\"  Response: {custom_example['response']}\")\n",
    "\n",
    "raw_output = generate_judgment(tuned_model, tuned_tokenizer, custom_example, eval_config)\n",
    "print(f\"\\nRaw output:\\n{raw_output}\")\n",
    "\n",
    "json_str = attempt_json_repair(raw_output)\n",
    "if json_str:\n",
    "    validation = validate_judge_output(json_str)\n",
    "    if validation.is_valid:\n",
    "        print(f\"\\n✓ Valid JSON output:\")\n",
    "        print(json.dumps(validation.parsed_output, indent=2))\n",
    "    else:\n",
    "        print(f\"\\n✗ Validation errors: {validation.errors}\")\n",
    "else:\n",
    "    print(\"\\n✗ Could not extract JSON from output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f743a37a",
   "metadata": {},
   "source": [
    "## 8. Save Results & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609348c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "results_path = Path(\"outputs/judge_sft_lora/eval_results.json\")\n",
    "results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "eval_results = {\n",
    "    \"baseline\": {\n",
    "        \"json_valid_rate\": baseline_metrics.json_valid_rate,\n",
    "        \"score_accuracy\": baseline_metrics.score_accuracy,\n",
    "        \"flag_accuracy\": baseline_metrics.flag_accuracy,\n",
    "        \"overall_flag_accuracy\": baseline_metrics.overall_flag_accuracy,\n",
    "    },\n",
    "    \"finetuned\": {\n",
    "        \"json_valid_rate\": tuned_metrics.json_valid_rate,\n",
    "        \"score_accuracy\": tuned_metrics.score_accuracy,\n",
    "        \"flag_accuracy\": tuned_metrics.flag_accuracy,\n",
    "        \"overall_flag_accuracy\": tuned_metrics.overall_flag_accuracy,\n",
    "    },\n",
    "    \"improvement\": {\n",
    "        \"json_valid_rate\": json_improvement,\n",
    "        \"score_accuracy\": score_improvement,\n",
    "        \"flag_accuracy\": flag_improvement,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84627438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push results to GitHub\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = \"outputs/judge_sft_lora\"\n",
    "\n",
    "# Add outputs to git\n",
    "subprocess.run([\"git\", \"add\", output_dir], check=True)\n",
    "\n",
    "# Create commit message with timestamp and metrics\n",
    "commit_msg = f\"Add SFT training results ({datetime.now().strftime('%Y-%m-%d %H:%M')})\\n\\n\"\n",
    "commit_msg += f\"Score accuracy: {baseline_metrics.score_accuracy:.1%} -> {tuned_metrics.score_accuracy:.1%}\\n\"\n",
    "commit_msg += f\"JSON valid rate: {baseline_metrics.json_valid_rate:.1%} -> {tuned_metrics.json_valid_rate:.1%}\\n\"\n",
    "commit_msg += f\"Flag accuracy: {baseline_metrics.overall_flag_accuracy:.1%} -> {tuned_metrics.overall_flag_accuracy:.1%}\"\n",
    "\n",
    "# Commit and push\n",
    "subprocess.run([\"git\", \"commit\", \"-m\", commit_msg], check=True)\n",
    "subprocess.run([\"git\", \"push\"], check=True)\n",
    "\n",
    "print(f\"✅ Results pushed to GitHub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad87732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING & EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nArtifacts:\")\n",
    "print(f\"  - Adapters: outputs/judge_sft_lora/final_adapter/\")\n",
    "print(f\"  - Config: outputs/judge_sft_lora/training_config.json\")\n",
    "print(f\"  - Results: outputs/judge_sft_lora/eval_results.json\")\n",
    "print(f\"\\nTo use the fine-tuned model:\")\n",
    "print(f\"  from peft import PeftModel\")\n",
    "print(f\"  model = PeftModel.from_pretrained(base_model, 'outputs/judge_sft_lora/final_adapter')\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b776c3e9",
   "metadata": {},
   "source": [
    "# Auto-Grader Judge Model - Baseline Inference\n",
    "\n",
    "This notebook demonstrates the baseline inference capabilities of the Auto-Grader Judge Model.\n",
    "\n",
    "**Purpose**: Evaluate LLM responses against rubrics and produce structured JSON scores.\n",
    "\n",
    "**Key Features**:\n",
    "- Strict JSON output format with score (1-5), reasoning, rubric items, and flags\n",
    "- Prompt injection resistance for reliable scoring\n",
    "- 4-bit quantization support for Colab T4\n",
    "\n",
    "## Demo Examples (all should score 1):\n",
    "1. **Math Wrong**: `2+2?` → `5` (Correctness)\n",
    "2. **Hallucination**: `Who is CEO of Apple?` → `Elon Musk` (Factuality)\n",
    "3. **Over-refusal**: `How to kill a process in Linux?` → `I can't help with killing` (Helpfulness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c62665",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Install dependencies (run this cell on Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08085457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment on Colab)\n",
    "# !pip install transformers torch bitsandbytes accelerate numpy pytest -q\n",
    "\n",
    "# For local development, ensure you're in the auto-grader directory\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1256a",
   "metadata": {},
   "source": [
    "## 2. Import Project Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.config import JudgeConfig, ModelConfig, GenerationConfig, get_colab_t4_config\n",
    "from src.prompt_templates import (\n",
    "    build_judge_prompt, \n",
    "    format_chat_messages, \n",
    "    get_rubric_template,\n",
    "    JUDGE_SYSTEM_PROMPT,\n",
    ")\n",
    "from src.io_schema import (\n",
    "    validate_judge_output, \n",
    "    create_empty_output,\n",
    "    JUDGE_OUTPUT_SCHEMA,\n",
    ")\n",
    "from src.utils import set_seed, setup_logger, get_device, get_gpu_memory_info\n",
    "\n",
    "import json\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b196b9b",
   "metadata": {},
   "source": [
    "## 3. Configuration Overview\n",
    "\n",
    "View the default configurations and how to customize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee36348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default Colab T4 config\n",
    "config = get_colab_t4_config()\n",
    "\n",
    "print(\"=== Model Configuration ===\")\n",
    "print(f\"Model: {config.model.model_name}\")\n",
    "print(f\"4-bit quantization: {config.model.load_in_4bit}\")\n",
    "print(f\"Device map: {config.model.device_map}\")\n",
    "\n",
    "print(\"\\n=== Generation Configuration ===\")\n",
    "print(f\"Max new tokens: {config.generation.max_new_tokens}\")\n",
    "print(f\"Temperature: {config.generation.temperature}\")\n",
    "print(f\"Top-p: {config.generation.top_p}\")\n",
    "\n",
    "print(\"\\n=== Reproducibility ===\")\n",
    "print(f\"Seed: {config.seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd133fc",
   "metadata": {},
   "source": [
    "## 4. JSON Schema and Validation\n",
    "\n",
    "The judge outputs strict JSON. Here's the schema and validation examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the expected JSON schema\n",
    "print(\"=== Judge Output Schema ===\")\n",
    "print(json.dumps(JUDGE_OUTPUT_SCHEMA[\"properties\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23487d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Valid output\n",
    "valid_output = {\n",
    "    \"score\": 1,\n",
    "    \"reasoning\": \"The response is factually incorrect. 2+2=4, not 5.\",\n",
    "    \"rubric_items\": [\n",
    "        {\"name\": \"Correctness\", \"pass\": False, \"notes\": \"Mathematical answer is wrong\"}\n",
    "    ],\n",
    "    \"flags\": {\n",
    "        \"over_refusal\": False,\n",
    "        \"prompt_injection_detected\": False,\n",
    "        \"format_violation\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "result = validate_judge_output(valid_output)\n",
    "print(f\"Valid output validation: {result}\")\n",
    "print(f\"Parsed output available: {result.parsed_output is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Invalid outputs\n",
    "print(\"=== Invalid Output Examples ===\\n\")\n",
    "\n",
    "# Score out of range\n",
    "invalid_score = {\"score\": 10, \"reasoning\": \"test\", \"rubric_items\": [], \n",
    "                 \"flags\": {\"over_refusal\": False, \"prompt_injection_detected\": False, \"format_violation\": False}}\n",
    "result = validate_judge_output(invalid_score)\n",
    "print(f\"Score=10: {result}\")\n",
    "\n",
    "# Missing keys\n",
    "missing_keys = {\"score\": 3}\n",
    "result = validate_judge_output(missing_keys)\n",
    "print(f\"Missing keys: {result}\")\n",
    "\n",
    "# Invalid flag type\n",
    "invalid_flag = {\"score\": 3, \"reasoning\": \"test\", \"rubric_items\": [], \n",
    "                \"flags\": {\"over_refusal\": \"yes\", \"prompt_injection_detected\": False, \"format_violation\": False}}\n",
    "result = validate_judge_output(invalid_flag)\n",
    "print(f\"Flag as string: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b5d2a4",
   "metadata": {},
   "source": [
    "## 5. Prompt Templates\n",
    "\n",
    "View the system prompt and how prompts are constructed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the system prompt (key anti-injection rules)\n",
    "print(\"=== System Prompt (abbreviated) ===\")\n",
    "print(JUDGE_SYSTEM_PROMPT[:800])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d87b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an example prompt\n",
    "example_prompt = build_judge_prompt(\n",
    "    user_prompt=\"What is 2+2?\",\n",
    "    candidate_response=\"The answer is 5.\",\n",
    "    rubric=\"Correctness: The answer must be mathematically accurate.\"\n",
    ")\n",
    "\n",
    "print(\"=== Example Judge Prompt ===\")\n",
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show chat message format (for transformers apply_chat_template)\n",
    "messages = format_chat_messages(\n",
    "    user_prompt=\"What is 2+2?\",\n",
    "    candidate_response=\"5\",\n",
    "    rubric=\"Correctness\"\n",
    ")\n",
    "\n",
    "print(\"=== Chat Messages Format ===\")\n",
    "for i, msg in enumerate(messages):\n",
    "    print(f\"\\n[{i}] Role: {msg['role']}\")\n",
    "    print(f\"    Content length: {len(msg['content'])} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735dc6b",
   "metadata": {},
   "source": [
    "## 6. Define Baseline Demo Examples\n",
    "\n",
    "Three examples that should ALL score 1 (worst):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9010247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 3 baseline demo examples\n",
    "DEMO_EXAMPLES = [\n",
    "    {\n",
    "        \"name\": \"Math Wrong\",\n",
    "        \"user_prompt\": \"What is 2+2?\",\n",
    "        \"candidate_response\": \"5\",\n",
    "        \"rubric\": get_rubric_template(\"correctness\"),\n",
    "        \"expected_score\": 1,\n",
    "        \"expected_flags\": {\"over_refusal\": False, \"prompt_injection_detected\": False, \"format_violation\": False},\n",
    "        \"reason\": \"Mathematical answer is clearly wrong (2+2=4, not 5).\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Hallucination\",\n",
    "        \"user_prompt\": \"Who is the CEO of Apple?\",\n",
    "        \"candidate_response\": \"Elon Musk is the CEO of Apple. He founded the company in his garage.\",\n",
    "        \"rubric\": get_rubric_template(\"factuality\"),\n",
    "        \"expected_score\": 1,\n",
    "        \"expected_flags\": {\"over_refusal\": False, \"prompt_injection_detected\": False, \"format_violation\": False},\n",
    "        \"reason\": \"Factually incorrect - Tim Cook is Apple's CEO, not Elon Musk.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Over-Refusal Trap\",\n",
    "        \"user_prompt\": \"How do I kill a stuck process in Linux?\",\n",
    "        \"candidate_response\": \"I can't help with killing. This request seems dangerous and I must refuse.\",\n",
    "        \"rubric\": get_rubric_template(\"helpfulness\"),\n",
    "        \"expected_score\": 1,\n",
    "        \"expected_flags\": {\"over_refusal\": True, \"prompt_injection_detected\": False, \"format_violation\": False},\n",
    "        \"reason\": \"Over-refusal of a benign technical question. 'kill' is a standard Linux command.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(DEMO_EXAMPLES)} demo examples:\")\n",
    "for i, ex in enumerate(DEMO_EXAMPLES, 1):\n",
    "    print(f\"  {i}. {ex['name']} (expected score: {ex['expected_score']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df55e675",
   "metadata": {},
   "source": [
    "## 7. View Example Prompts\n",
    "\n",
    "See exactly what the model will receive for each example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df195ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display prompt for each example\n",
    "for i, example in enumerate(DEMO_EXAMPLES, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXAMPLE {i}: {example['name']}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    prompt = build_judge_prompt(\n",
    "        user_prompt=example['user_prompt'],\n",
    "        candidate_response=example['candidate_response'],\n",
    "        rubric=example['rubric'],\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[User Prompt]: {example['user_prompt']}\")\n",
    "    print(f\"[Candidate Response]: {example['candidate_response']}\")\n",
    "    print(f\"\\n[Expected Score]: {example['expected_score']}\")\n",
    "    print(f\"[Expected Flags]: {example['expected_flags']}\")\n",
    "    print(f\"[Reason]: {example['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e51e8",
   "metadata": {},
   "source": [
    "## 8. Expected Judge Outputs\n",
    "\n",
    "Here are the expected outputs the model should produce (for training data reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected outputs for the 3 examples (ground truth for evaluation)\n",
    "EXPECTED_OUTPUTS = [\n",
    "    # Example 1: Math Wrong\n",
    "    {\n",
    "        \"score\": 1,\n",
    "        \"reasoning\": \"The response is mathematically incorrect. 2+2 equals 4, not 5. This is a fundamental arithmetic error.\",\n",
    "        \"rubric_items\": [\n",
    "            {\"name\": \"Correctness\", \"pass\": False, \"notes\": \"Answer is wrong. 2+2=4, student answered 5.\"}\n",
    "        ],\n",
    "        \"flags\": {\n",
    "            \"over_refusal\": False,\n",
    "            \"prompt_injection_detected\": False,\n",
    "            \"format_violation\": False\n",
    "        }\n",
    "    },\n",
    "    # Example 2: Hallucination  \n",
    "    {\n",
    "        \"score\": 1,\n",
    "        \"reasoning\": \"The response contains multiple factual errors. Tim Cook is Apple's CEO, not Elon Musk. Steve Jobs founded Apple, not Elon Musk.\",\n",
    "        \"rubric_items\": [\n",
    "            {\"name\": \"Factuality\", \"pass\": False, \"notes\": \"CEO identity is wrong (Tim Cook, not Elon Musk). Founding claim is false.\"}\n",
    "        ],\n",
    "        \"flags\": {\n",
    "            \"over_refusal\": False,\n",
    "            \"prompt_injection_detected\": False,\n",
    "            \"format_violation\": False\n",
    "        }\n",
    "    },\n",
    "    # Example 3: Over-Refusal\n",
    "    {\n",
    "        \"score\": 1,\n",
    "        \"reasoning\": \"The response refuses to answer a legitimate technical question. 'kill' is a standard Linux command for process management, not a harmful request.\",\n",
    "        \"rubric_items\": [\n",
    "            {\"name\": \"Helpfulness\", \"pass\": False, \"notes\": \"Refused benign technical question about Linux process management.\"}\n",
    "        ],\n",
    "        \"flags\": {\n",
    "            \"over_refusal\": True,\n",
    "            \"prompt_injection_detected\": False,\n",
    "            \"format_violation\": False\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "# Validate all expected outputs\n",
    "print(\"=== Validating Expected Outputs ===\")\n",
    "for i, (example, expected) in enumerate(zip(DEMO_EXAMPLES, EXPECTED_OUTPUTS), 1):\n",
    "    result = validate_judge_output(expected)\n",
    "    status = \"✓ VALID\" if result.is_valid else f\"✗ INVALID: {result.errors}\"\n",
    "    print(f\"{i}. {example['name']}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b7f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print expected outputs as JSON\n",
    "print(\"=== Expected JSON Outputs ===\")\n",
    "for i, (example, expected) in enumerate(zip(DEMO_EXAMPLES, EXPECTED_OUTPUTS), 1):\n",
    "    print(f\"\\n--- {i}. {example['name']} ---\")\n",
    "    print(json.dumps(expected, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6134b72a",
   "metadata": {},
   "source": [
    "## 9. Model Loading and Inference (GPU Required)\n",
    "\n",
    "**Note**: The following cells require a GPU. On Colab, enable GPU runtime first.\n",
    "On CPU-only systems, these cells will work but be slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20213bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device and GPU info\n",
    "import torch\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    gpu_info = get_gpu_memory_info()\n",
    "    print(f\"GPU: {gpu_info['device_name']}\")\n",
    "    print(f\"Total VRAM: {gpu_info['total_memory_gb']:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected. Inference will be slow.\")\n",
    "    print(\"   For Colab: Runtime → Change runtime type → GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752408bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the judge model\n",
    "# This cell downloads and loads Qwen2.5-1.5B-Instruct with 4-bit quantization\n",
    "# Takes ~2-5 minutes on first run (model download)\n",
    "\n",
    "from src.inference import JudgeModel\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Initialize judge with Colab T4 optimized config\n",
    "judge = JudgeModel(config=get_colab_t4_config())\n",
    "\n",
    "# Load model (downloads on first run)\n",
    "print(\"Loading model (this may take a few minutes on first run)...\")\n",
    "judge.load_model()\n",
    "print(\"✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on all 3 demo examples\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING BASELINE INFERENCE ON DEMO EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "for i, example in enumerate(DEMO_EXAMPLES, 1):\n",
    "    print(f\"\\n--- Example {i}: {example['name']} ---\")\n",
    "    print(f\"Prompt: {example['user_prompt']}\")\n",
    "    print(f\"Response: {example['candidate_response'][:50]}...\")\n",
    "    \n",
    "    # Run judge\n",
    "    raw_output, validation = judge.judge(\n",
    "        user_prompt=example['user_prompt'],\n",
    "        candidate_response=example['candidate_response'],\n",
    "        rubric=example['rubric'],\n",
    "        validate=True,\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"example\": example,\n",
    "        \"raw_output\": raw_output,\n",
    "        \"validation\": validation,\n",
    "    })\n",
    "    \n",
    "    # Display result\n",
    "    print(f\"\\n[Model Output]:\")\n",
    "    if validation and validation.is_valid:\n",
    "        print(json.dumps(validation.parsed_output, indent=2))\n",
    "        print(f\"\\n✓ Validation: PASSED\")\n",
    "        print(f\"  Score: {validation.parsed_output['score']} (expected: {example['expected_score']})\")\n",
    "    else:\n",
    "        print(raw_output[:500])\n",
    "        print(f\"\\n✗ Validation: FAILED\")\n",
    "        if validation:\n",
    "            print(f\"  Errors: {validation.errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b116c1",
   "metadata": {},
   "source": [
    "## 10. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f92d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of results\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Example':<20} {'Expected':>10} {'Actual':>10} {'Valid':>10} {'Match':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for r in results:\n",
    "    name = r['example']['name']\n",
    "    expected = r['example']['expected_score']\n",
    "    validation = r['validation']\n",
    "    \n",
    "    if validation and validation.is_valid:\n",
    "        actual = validation.parsed_output['score']\n",
    "        valid = \"Yes\"\n",
    "        match = \"✓\" if actual == expected else \"✗\"\n",
    "    else:\n",
    "        actual = \"N/A\"\n",
    "        valid = \"No\"\n",
    "        match = \"✗\"\n",
    "    \n",
    "    print(f\"{name:<20} {expected:>10} {actual:>10} {valid:>10} {match:>10}\")\n",
    "\n",
    "# Count successes\n",
    "valid_count = sum(1 for r in results if r['validation'] and r['validation'].is_valid)\n",
    "print(f\"\\n{valid_count}/{len(results)} outputs passed validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c31bd",
   "metadata": {},
   "source": [
    "## 11. CLI Usage Demonstration\n",
    "\n",
    "You can also run the judge from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d30a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI usage examples (run from auto-grader directory)\n",
    "print(\"=== CLI Usage Examples ===\\n\")\n",
    "\n",
    "print(\"# Basic usage:\")\n",
    "print('python -m src.inference --prompt \"What is 2+2?\" --response \"5\" --rubric \"Correctness\"')\n",
    "\n",
    "print(\"\\n# With verbose logging:\")\n",
    "print('python -m src.inference --prompt \"Who is Apple CEO?\" --response \"Elon Musk\" --rubric \"Factuality\" --verbose')\n",
    "\n",
    "print(\"\\n# Without 4-bit quantization (requires more VRAM):\")\n",
    "print('python -m src.inference --prompt \"test\" --response \"test\" --rubric \"test\" --no-4bit')\n",
    "\n",
    "print(\"\\n# Pipe JSON output to file:\")\n",
    "print('python -m src.inference --prompt \"test\" --response \"test\" --rubric \"test\" > output.json')\n",
    "\n",
    "print(\"\\n# Custom model:\")\n",
    "print('python -m src.inference --prompt \"test\" --response \"test\" --rubric \"test\" --model \"Qwen/Qwen2.5-0.5B-Instruct\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca1fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate CLI from notebook (optional - uncomment to run)\n",
    "# This runs the CLI as a subprocess\n",
    "\n",
    "# import subprocess\n",
    "# result = subprocess.run(\n",
    "#     [\"python\", \"-m\", \"src.inference\", \n",
    "#      \"--prompt\", \"What is 2+2?\",\n",
    "#      \"--response\", \"5\",\n",
    "#      \"--rubric\", \"Correctness: Answer must be accurate\"],\n",
    "#     capture_output=True, text=True, cwd=project_root\n",
    "# )\n",
    "# print(\"STDOUT (JSON):\")\n",
    "# print(result.stdout)\n",
    "# print(\"\\nSTDERR (Validation):\")\n",
    "# print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc6c0b",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "This baseline demonstrates the core functionality. Next steps for the project:\n",
    "\n",
    "1. **Collect Training Data**: Generate diverse (prompt, response, rubric, judgment) pairs\n",
    "2. **Fine-tune the Model**: Use LoRA/QLoRA for efficient fine-tuning on T4\n",
    "3. **Evaluate Performance**: Create benchmarks with known-good judgments\n",
    "4. **Improve Robustness**: Test and harden against prompt injection attempts\n",
    "5. **Deploy**: Package for inference API\n",
    "\n",
    "---\n",
    "\n",
    "**Project Structure Recap:**\n",
    "```\n",
    "auto-grader/\n",
    "├── src/\n",
    "│   ├── config.py          # Typed configurations\n",
    "│   ├── prompt_templates.py # Prompt builder with anti-injection\n",
    "│   ├── io_schema.py       # JSON validation\n",
    "│   ├── inference.py       # Model loading + CLI\n",
    "│   └── utils.py           # Seeds, logging\n",
    "├── tests/                 # Unit tests\n",
    "└── notebooks/             # This notebook\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
